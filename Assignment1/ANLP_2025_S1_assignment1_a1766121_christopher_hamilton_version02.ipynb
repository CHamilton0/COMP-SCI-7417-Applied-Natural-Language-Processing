{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6730fecf",
   "metadata": {},
   "source": [
    "## ANLP Assignment 1: Sentiment Analysis\n",
    "### Christopher Hamilton,  a1766121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a632e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1fad6c-9f9a-431c-86fc-c40547146582",
   "metadata": {},
   "source": [
    "### 1. Reading dataset and initial pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_to_df(file_name):\n",
    "    data = []\n",
    "    with open(file_name) as data_file:\n",
    "        for line in data_file:\n",
    "            # Load each line of the JSON file as a dictionary\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    # Form a Pandas DataFrame from the dictionaries\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "# Load the training and test data\n",
    "raw_train_df = read_json_to_df(\"hotel_reviews_train.json\")\n",
    "raw_test_df = read_json_to_df(\"hotel_reviews_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the initially loaded dataframes\n",
    "raw_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the title, text and overall rating columns to make a new dataframe\n",
    "train_df = raw_train_df[[\"title\", \"text\", \"ratings.overall\"]]\n",
    "test_df = raw_test_df[[\"title\", \"text\", \"ratings.overall\"]]\n",
    "\n",
    "# Check the value counts for the ratings\n",
    "print(\"Training data ratings\")\n",
    "print(train_df[\"ratings.overall\"].value_counts())\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Test data ratings\")\n",
    "print(test_df[\"ratings.overall\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc754ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of rows where the rating is 0\n",
    "zero_rating_indices = test_df[test_df['ratings.overall'] == 0].index\n",
    "for index in zero_rating_indices:\n",
    "    # Print the text corresponding to the zero rating\n",
    "    print(test_df['text'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e362cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the above text, it is unlikely the reviewer meant to give a low rating\n",
    "# Instead, we will remvoe the 0 from the dataset\n",
    "test_df = test_df.drop(zero_rating_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f413ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value counts for the ratings after the 0 rating has been removed\n",
    "print(\"Test data ratings\")\n",
    "print(test_df[\"ratings.overall\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e79a19",
   "metadata": {},
   "source": [
    "Python's lambda functions can be used to remove the special characters from the dataset. Pandas DataFrames columns include an `apply` method that can take in a lambda function to apply to each cell in the column. By including a lambda function that will only include characters which are alphanumeric or spaces, the special characters can be removed from the dataset (Saturn Cloud 2024).\n",
    "\n",
    "At the same time, we can apply the `lower()` function on each character to convert all the text to lowercase. This can be seen by viewing the first few rows with the `head()` function on the DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9df57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove remove non-alphanumeric characters from the title and text columns\n",
    "train_df.loc[:, 'title'] = train_df['title'].apply(lambda x: ''.join(char.lower() for char in x if char.isalnum() or char.isspace()))\n",
    "train_df.loc[:, 'text'] = train_df['text'].apply(lambda x: ''.join(char.lower() for char in x if char.isalnum() or char.isspace()))\n",
    "\n",
    "test_df.loc[:, 'title'] = test_df['title'].apply(lambda x: ''.join(char.lower() for char in x if char.isalnum() or char.isspace()))\n",
    "test_df.loc[:, 'text'] = test_df['text'].apply(lambda x: ''.join(char.lower() for char in x if char.isalnum() or char.isspace()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2364efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ac8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb15948",
   "metadata": {},
   "source": [
    "The provided code for the `language_filter.py` file includes an example of using the `langdetect` Python package to filter for only English text. Rather than applying the filter for only English reviews when reading the file, we can apply the filter on the loaded DataFrames using a similar method to above. By using the Pandas `apply` method on the text and title columns, the returned DataFrame will only include rows where both the title and text are in English as determined by the `langdetect` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb99b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect as detect_language\n",
    "\n",
    "def filter_english_reviews(df):\n",
    "    def is_english(text):\n",
    "        try:\n",
    "            return detect_language(text) == \"en\"\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    # Filter the DataFrame for reviews where both title and text are in English\n",
    "    return df[df['text'].apply(is_english) & df['title'].apply(is_english)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ac11e",
   "metadata": {},
   "source": [
    "Since the language detecting process takes some time over the whole dataset, to save time during development, the filtered DataFrames can be saved and loaded from CSV. Since these DataFrames will not change, and all preprocessing steps are the same, running the language filter each time is not necessary. I have written some quick checks to see if the files have already been saved, and if they have load them, otherwise run the language check code and save the files for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the English reviews to a CSV file to save time filtering when running again (NumFOCUS, Inc. 2024)\n",
    "if os.path.exists(\"english_hotel_reviews_train.csv\"):\n",
    "    train_df = pd.read_csv(\"english_hotel_reviews_train.csv\")\n",
    "else:\n",
    "    train_df = filter_english_reviews(train_df)\n",
    "    train_df.to_csv(\"english_hotel_reviews_train.csv\", index=False)\n",
    "\n",
    "if os.path.exists(\"english_hotel_reviews_test.csv\"):\n",
    "    test_df = pd.read_csv(\"english_hotel_reviews_test.csv\")\n",
    "else:\n",
    "    test_df = filter_english_reviews(test_df)\n",
    "    test_df.to_csv(\"english_hotel_reviews_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d75afc",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ce118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot distribution of ratings\n",
    "train_df['ratings.overall'].value_counts().sort_index().plot(kind='bar', figsize=(8,5), color='skyblue')\n",
    "\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce55f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot distribution of ratings\n",
    "test_df['ratings.overall'].value_counts().sort_index().plot(kind='bar', figsize=(8,5), color='green')\n",
    "\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a1d58",
   "metadata": {},
   "source": [
    "The distribution of the ratings can be plotted on a bar chart for both the training and test data. From the charts above, it is clear that most of the ratings for the hotels in the hotel booking company are positive, with a similar distribution of ratings across the training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38259a9",
   "metadata": {},
   "source": [
    "Based on the code provided as part of Workshop 2, the predictive and non-predictive words in the dataset can be found using the TF-IDF (Term Frequency-Inverse Document Frequency) (Feature Engineering 2025). From TF-IDF, the words with the correlations closest to 0 indicate a very small effect on the prediction, whereas the words with a correlation higher indicate they are more positive and words with a more negative correlation indicate they are more negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684bc013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(tf_idf_train.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Find the correlations with the ratings\n",
    "correlations = tfidf_df.corrwith(train_df[\"ratings.overall\"])\n",
    "correlations = correlations.sort_values(ascending=False)\n",
    "\n",
    "# Find 10 words with the weakest correlation by sorting\n",
    "non_predictive_words = correlations.sort_values(key=lambda x: np.abs(x))\n",
    "print(\"Non-Predictive Words:\\n\", non_predictive_words.head(10))\n",
    "\n",
    "# Display top 10 positive and negative correlated words\n",
    "print(\"Most Positive Words:\\n\", correlations.head(10))\n",
    "print(\"\\nMost Negative Words:\\n\", correlations.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1224548",
   "metadata": {},
   "source": [
    "In order to find the number of unique words, the text can be converted into a list of tokens, and the number of unique tokens can then easily be found with `numpy`. Given that the data to be used for classification into the ratings is the textual review data, the title and text columns can be combined into a single text column. To make analysis simpler, the overall rating column can also be renamed to just rating. At this stage the stop words are also removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726be157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Create a column with the title and text together\n",
    "train_df[\"combined_text\"] = train_df[\"title\"] + \" \" + train_df[\"text\"]\n",
    "test_df[\"combined_text\"] = test_df[\"title\"] + \" \" + test_df[\"text\"]\n",
    "\n",
    "train_df = train_df.drop(columns=[\"title\", \"text\"])\n",
    "test_df = test_df.drop(columns=[\"title\", \"text\"])\n",
    "train_df = train_df.rename(columns={\"ratings.overall\": \"rating\", \"combined_text\": \"text\"})\n",
    "test_df = test_df.rename(columns={\"ratings.overall\": \"rating\", \"combined_text\": \"text\"})\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda text: ' '.join([word for word in text.split(' ') if word not in stop_words]))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda text: ' '.join([word for word in text.split(' ') if word not in stop_words]))\n",
    "\n",
    "# Split all reviews into words and find unique ones\n",
    "all_words_text = np.concatenate(train_df.text.apply(nltk.word_tokenize).to_numpy())\n",
    "\n",
    "unique_words = np.unique(all_words_text)\n",
    "\n",
    "print(\"Total Unique Words:\", len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d7e96",
   "metadata": {},
   "source": [
    "The most frequent words in the dataset can be plotted on a bar chart. Stop words are removed for this analysis so that the chart is not filled with very common words such as 'the' or 'is'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokens = [word for word in all_words_text if word not in stop_words]\n",
    "word_freq = Counter(tokens)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(*zip(*word_freq.most_common(20)))\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Word Frequency Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf51905",
   "metadata": {},
   "source": [
    "The most common trigrams in the dataset can give us insight into common phrases that are used in the dataset. (Exploratory Data Analysis 2025) These sequences can be calculated and listed as well as plotted on a chart for viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to generate n-grams\n",
    "def generate_ngrams(text, n):\n",
    "    n_grams = ngrams(text, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "# Specify the value of n for n-grams\n",
    "n_value = 3\n",
    "\n",
    "# Generate n-grams\n",
    "ngrams_list = generate_ngrams(tokens, n_value)\n",
    "\n",
    "# Count the occurrences of each n-gram\n",
    "ngrams_count = Counter(ngrams_list)\n",
    "most_common_ngrams = ngrams_count.most_common(100)\n",
    "\n",
    "# Display the distribution\n",
    "print(f\"Distribution of {n_value}-grams:\")\n",
    "for ngram, count in most_common_ngrams:\n",
    "    print(f\"{ngram}: {count}\")\n",
    "\n",
    "# Plot the distribution\n",
    "labels, values = zip(*most_common_ngrams)\n",
    "indexes = range(len(labels))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(indexes, values)\n",
    "plt.xlabel(f'{n_value}-grams')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(indexes, labels, rotation='vertical')\n",
    "plt.title(f'Distribution of {n_value}-grams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333cefa8",
   "metadata": {},
   "source": [
    "### 3. Selection and training Machine Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae3c14",
   "metadata": {},
   "source": [
    "When training machine learning models, the dataset should be balanced to ensure that there is no bias to any one category. In the training dataset, there are more positive reviews than negative, and as a result the trained model may become biased towards classifying text positively. To address this, it is possible to use oversampling to create a data set for training that includes an equal number for each category. (Income Evaluation Notebook 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the training data by oversampling\n",
    "def balance_data_oversample(df):\n",
    "    max_count = df['rating'].value_counts().max()\n",
    "    balanced_df = pd.DataFrame()\n",
    "\n",
    "    for rating in df['rating'].unique():\n",
    "        rating_df = df[df['rating'] == rating]\n",
    "        balanced_df = pd.concat([balanced_df, rating_df.sample(max_count, replace=True)])\n",
    "\n",
    "    return balanced_df\n",
    "balanced_train_df = balance_data_oversample(train_df)\n",
    "\n",
    "# Plot distribution of ratings\n",
    "balanced_train_df['rating'].value_counts().sort_index().plot(kind='bar', figsize=(8,5), color='skyblue')\n",
    "\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a86e1",
   "metadata": {},
   "source": [
    "The text is already in lowercase and stop words have been removed from the dataset. To prepare the data for machine learning, the text can be lemmatised. Lemmatisation is one method for reducing words to their base forms, and this can be included in the preprocessing of data before a machine learning technique is applied to improve results. (Murel 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a28134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "balanced_train_df.loc[:, 'text'] = balanced_train_df['text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))\n",
    "test_df.loc[:, 'text'] = test_df['text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd4bd9",
   "metadata": {},
   "source": [
    "The classical machine learning method that will be used in this experiment in Multinomial Naive Bayes. This classification algorithm \"simplifies the process of classifying text by assuming that the presence of one word doesn’t depend on others\", which \"makes it computationally efficient and reliable for a range of tasks\" (Sriram 2024). In order to train the Multinomial Naive Bayes classifier, the data must be arranged into a training and validation set.\n",
    "\n",
    "The Scikit Learn Python module includes a function to automatically split a dataset into a training and testing set or a training an validation set. For the training that is to be completed in this experiment, 80% of the data will be used for training and 20% will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cb94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_res = balanced_train_df[\"text\"]\n",
    "y_res = balanced_train_df[\"rating\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_res, y_res, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Feature Engineering 2025)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_val_vectors = vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db783c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f580e54",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e89d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9755c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Income Evaluation Notebook 2025)\n",
    "nb_accuracies = cross_val_score(classifier, X_train_vectors, y_train, cv=5)\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "print(f\"Naive Bayes Train Score: {round(np.mean(nb_accuracies) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_score = classifier.score(X_val_vectors, y_val)\n",
    "print(f\"Naive Bayes Validation Score: {round(naive_bayes_score * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9f559",
   "metadata": {},
   "source": [
    "After training the Multinomial Naive Bayes classifier on the training data and testing the accuracy on the validation data, it is clear that the classification has performed quite well. The accuracy percentages are shown above, and this model could be considered to evaluate using the test data as well. However, a deep learning model should also be trained to determine how well it performs.\n",
    "\n",
    "To do this, Tensorflow and Keras will be used. Some extra configuration is needed for Tensorflow to make use of the GPU, without encountering memory issues, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Limit GPU memory usage\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.LogicalDeviceConfiguration(memory_limit=(6 * 1024))])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c1c7f6",
   "metadata": {},
   "source": [
    "Since the problem to be solved is to classify text data into one of 5 rating categories, it may make sense to use a classification model. However, the problem is also to understand how reliable the ratings are, and therefore it may be useful to understand how different the model's prediciton is compared to the actual rating.\n",
    "\n",
    "To do this, a regression model will be used. The same text that the Multinomial Naive Bayes algorithm was trained on will be used for trainin the regression model, and as outlined by Poliak, the GloVe (Global Vectors for Word Representation) can be used to represent the words in the text for the machine learning model (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b483281",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = balanced_train_df[\"rating\"]\n",
    "\n",
    "test_Y = test_df[\"rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e55900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Store the GloVe files in a directory in this repository\n",
    "glove_dir = '../glove'\n",
    "if not os.path.exists(glove_dir):\n",
    "    os.makedirs(glove_dir)\n",
    "\n",
    "glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "glove_zip_path = os.path.join(glove_dir, \"glove.6B.zip\")\n",
    "\n",
    "# Download the GloVe file\n",
    "if not os.path.exists(glove_zip_path):\n",
    "    print(\"Downloading GloVe embeddings...\")\n",
    "    # (Reitz 2016)\n",
    "    response = requests.get(glove_url, stream=True)\n",
    "    with open(glove_zip_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# Extract the GloVe file\n",
    "if not os.path.exists(os.path.join(glove_dir, \"glove.6B.100d.txt\")):\n",
    "    print(\"Extracting GloVe embeddings...\")\n",
    "    with zipfile.ZipFile(glove_zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(glove_dir)\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "# (Poliak 2020)\n",
    "embedding_index = {}\n",
    "f = open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors ' % len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2fec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# (Poliak 2020)\n",
    "tokenizer=Tokenizer(oov_token=\"'oov'\")\n",
    "tokenizer.fit_on_texts(balanced_train_df['text'])\n",
    "\n",
    "max_words = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words,embedding_dim))\n",
    "\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx]=embedding_vector\n",
    "\n",
    "maxlen = 200\n",
    "train_X = pad_sequences(tokenizer.texts_to_sequences(balanced_train_df['text']), maxlen=maxlen)\n",
    "test_X = pad_sequences(tokenizer.texts_to_sequences(test_df['text']), maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75de5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "def create_regression_model():\n",
    "    # Define a regression model\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, weights=[embedding_matrix], trainable=False))\n",
    "    model.add(Bidirectional(LSTM(8)))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_regression_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "model.build(train_X.shape)\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model with 20% of data used for validation\n",
    "history = model.fit(\n",
    "    train_X,\n",
    "    train_Y,\n",
    "    epochs=25,\n",
    "    batch_size=256,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d175e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot the training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history.history['loss'], label='Train MSE')\n",
    "plt.plot(history.history['val_loss'], label='Validation MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Model Training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9335c",
   "metadata": {},
   "source": [
    "As shown in the training and the graph above, the model was trained successfully, with both the training loss and validation loss decreasing over the time spent training. Testing will need to be done with this model for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cebcf",
   "metadata": {},
   "source": [
    "### 4. Experiment with VADER sentiment lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c816a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def include_sentiment_analysis(df):\n",
    "    df2 = df.copy()\n",
    "    # Create text data from text and title\n",
    "    text_data = df2[\"text\"].to_numpy()\n",
    "\n",
    "    # Create target vector for VADER. Define a rating of 4 or 5 to be positive, 1 or 2 to be negative and 3 to be neutral\n",
    "    y = train_Y.apply(lambda x: \"positive\" if x > 3 else (\"negative\" if x < 3 else \"neutral\")).tolist()\n",
    "\n",
    "    # Analyse with VADER\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # (VADER Sentiment Example 2025)\n",
    "    for text in text_data:\n",
    "        score = analyser.polarity_scores(text)\n",
    "        sentiment = \"neutral\"\n",
    "        # Classify the sentiment based on the compound score from the analyser\n",
    "        if score['compound'] > 0.05:\n",
    "            sentiment = \"positive\"\n",
    "        elif score['compound'] < -0.05:\n",
    "            sentiment = \"negative\"\n",
    "        \n",
    "        # Compare the predicted sentiment with the actual sentiment\n",
    "        index = text_data.tolist().index(text)\n",
    "        if sentiment == y[index]:\n",
    "            correct_predictions += 1\n",
    "        # Add the score to the balanced_train_df in a new column\n",
    "        df2.loc[df2[\"text\"] == text, \"VADER_Sentiment\"] = sentiment\n",
    "\n",
    "    print(f\"VADER accuracy: {round(correct_predictions/len(text_data) * 100, 2)}%\")\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d81ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train_df2 = include_sentiment_analysis(balanced_train_df)\n",
    "train_X = pad_sequences(tokenizer.texts_to_sequences(balanced_train_df2['text']), maxlen=maxlen)\n",
    "# Create a training set with the vader sentiment represented as -1 if neutral, 0 if negative and 1 if positive\n",
    "train_X = np.concatenate((train_X, np.array(balanced_train_df2[\"VADER_Sentiment\"].apply(lambda x: 1 if x == \"positive\" else (-1 if x == \"negative\" else 0)).tolist()).reshape(-1, 1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb8126",
   "metadata": {},
   "source": [
    "In order to make use of the VADER sentiment analysis in this experiment, an assumption is made that the ratings which are rated higher would have more positive text, and lower ratings would have more negative text. However, after running the VADER sentiment analysis code over the training data, only 54.1% of the training data was classified correctly by VADER into positive, negative, or neutral, where positive was equivalent to ratings of 4 or 5, neutral was equivalent to a rating of 3, and negative was equivalent to a rating of 1 or 2.\n",
    "\n",
    "This may indicate that the ratings in the dataset are not reliable, since it is unlikely that positive words in a rating would result in a lower score, and vice-versa. However, the VADER Sentiment was added to the training dataset anyway to allow the regression model to train with it as an input too. A numerical value was assigned, with 1 being if the text was positive, 0 if neutral and -1 if the text was negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882cb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_model = create_regression_model()\n",
    "\n",
    "vader_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "vader_model.build(train_X.shape)\n",
    "\n",
    "# Train the model\n",
    "history = vader_model.fit(\n",
    "    train_X,\n",
    "    train_Y,\n",
    "    epochs=25,\n",
    "    batch_size=256,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e9782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot the training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history.history['loss'], label='Train MSE')\n",
    "plt.plot(history.history['val_loss'], label='Validation MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Model Training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ba5cb",
   "metadata": {},
   "source": [
    "### 5. Final testing on test set and discussion of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47316868",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the ratings for the test set and check the value compared to the actual ratings\n",
    "predictions = model.predict(test_X)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = np.mean((predictions.flatten() - test_Y.to_numpy().flatten())**2)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "# Round to the nearest whole number for the prediction\n",
    "predictions = np.round(predictions).astype(int)\n",
    "\n",
    "correct_predictions = np.sum(predictions.flatten() == test_Y.to_numpy().flatten())\n",
    "total_predictions = len(predictions)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the predictions which were incorrect by more than 1\n",
    "incorrect_predictions = np.abs(predictions.flatten() - test_Y.to_numpy().flatten()) > 1\n",
    "incorrect_reviews = test_df[incorrect_predictions]\n",
    "print(\"Incorrect Predictions:\")\n",
    "for i, row in incorrect_reviews.iterrows():\n",
    "    print(f\"Text: {row['text']}\")\n",
    "    print(f\"Predicted Rating: {predictions[i][0]}\")\n",
    "    print(f\"Actual Rating: {row['rating']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print the number of incorrect predictions compared to the total number of predictions\n",
    "num_incorrect = len(incorrect_reviews)\n",
    "num_total = len(test_df)\n",
    "print(f\"Total Predictions: {num_total}\")\n",
    "print(f\"Number of Correct Predictions: {num_total - num_incorrect}\")\n",
    "print(f\"Number of Incorrect Predictions: {num_incorrect}\")\n",
    "# Print the accuracy based on the number of correct predictions\n",
    "accuracy = (num_total - num_incorrect) / num_total\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6ebe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba7ea3a2",
   "metadata": {},
   "source": [
    "### 6. Propose a method to predict aspects \n",
    "\n",
    "***(COMP SCI 7417 and COMP SCI 7717 only)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cf2087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23d3d17f",
   "metadata": {},
   "source": [
    "### 7. Reflection on the ***Product*** development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4dae7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af38d9fc",
   "metadata": {},
   "source": [
    "### 9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3a062",
   "metadata": {},
   "source": [
    "'Exploratory Data Analysis', Applied Natural Language Processing workshop 2 code files, The University of Adelaide, in Week 2, Semester 1, 2025.\n",
    "\n",
    "'Feature Engineering', Applied Natural Language Processing workshop 2 code files, The University of Adelaide, in Week 2, Semester 1, 2025.\n",
    "\n",
    "'VADER Sentiment Example', Applied Natural Language Processing Assignment 1 code files, The University of Adelaide, in Semester 1, 2025.\n",
    "\n",
    "'Income Evaluation Notebook', Mining Big Data workshop 1 code files, The University of Adelaide, in Week 1, Semester 1, 2025.\n",
    "\n",
    "malamahadevan, 2025, Step-by-Step Exploratory Data Analysis (EDA) using Python, Analytics Vidhya, viewed 24 Mar 2025 <https://www.analyticsvidhya.com/blog/2022/07/step-by-step-exploratory-data-analysis-eda-using-python/>\n",
    "\n",
    "Murel, J, Kavlakoglu, E, 2023, What are stemming and lemmatization?, IBM, viewed 01 Apr 2025, <https://www.ibm.com/think/topics/stemming-lemmatization>\n",
    "\n",
    "NumFOCUS, Inc., 2024, pandas.DataFrame.to_csv, pandas, viewed 29 Mar 2025, <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html>\n",
    "\n",
    "NumFOCUS, Inc., 2024, pandas.read_csv, pandas, viewed 29 Mar 2025, <https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html>\n",
    "\n",
    "NumFOCUS, Inc., 2024, pandas.json_normalize, pandas, viewed 29 Mar 2025, <https://pandas.pydata.org/pandas-docs/version/1.2.0/reference/api/pandas.json_normalize.html>\n",
    "\n",
    "Saturn Cloud, 2024, How to Remove Special Characters in Pandas Dataframe, Saturn Cloud, viewed 29 Mar 2025, <https://saturncloud.io/blog/how-to-remove-special-characters-in-pandas-dataframe/#use-lambda-function>\n",
    "\n",
    "Sriram, 2024, Multinomial Naive Bayes Explained: Function, Advantages & Disadvantages, Applications, UpGrad, viewed 3 Apr 2025, <https://www.upgrad.com/blog/multinomial-naive-bayes-explained/>\n",
    "\n",
    "Poliak, S, 2020, 1 to 5 Star Ratings – Classification or Regression?, towards data science, viewed 29 Mar 2025, <https://towardsdatascience.com/1-to-5-star-ratings-classification-or-regression-b0462708a4df/>\n",
    "\n",
    "Reitz, K, 2016, Raw Response Content, Requests Documentation, viewed 29 Mar 2025, <https://requests.readthedocs.io/en/latest/user/quickstart/#raw-response-content>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c088010b-92f5-499b-a24e-4e3f874f4950",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7289b0-b12b-4be4-bc07-0b41a55266c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
