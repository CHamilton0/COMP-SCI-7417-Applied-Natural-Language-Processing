{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6730fecf",
   "metadata": {},
   "source": [
    "## ANLP Assignment 1: Sentiment Analysis\n",
    "### Christopher Hamilton,  a1766121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a632e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect as detect_language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1fad6c-9f9a-431c-86fc-c40547146582",
   "metadata": {},
   "source": [
    "### 1. Reading dataset and initial pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4836feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_to_df(file_name):\n",
    "    data = []\n",
    "    with open(file_name) as data_file:\n",
    "        for line in data_file:\n",
    "            data.append(json.loads(line))    \n",
    "\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "train_df = read_json_to_df(\"hotel_reviews_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2957279a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date_stayed</th>\n",
       "      <th>offering_id</th>\n",
       "      <th>num_helpful_votes</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>via_mobile</th>\n",
       "      <th>ratings.service</th>\n",
       "      <th>ratings.cleanliness</th>\n",
       "      <th>...</th>\n",
       "      <th>ratings.rooms</th>\n",
       "      <th>author.username</th>\n",
       "      <th>author.num_reviews</th>\n",
       "      <th>author.id</th>\n",
       "      <th>author.location</th>\n",
       "      <th>author.num_cities</th>\n",
       "      <th>author.num_helpful_votes</th>\n",
       "      <th>author.num_type_reviews</th>\n",
       "      <th>ratings.check_in_front_desk</th>\n",
       "      <th>ratings.business_service_(e_g_internet_access)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Horrible experience”</td>\n",
       "      <td>First of all we got there and they didn't have...</td>\n",
       "      <td>September 2012</td>\n",
       "      <td>80138</td>\n",
       "      <td>0</td>\n",
       "      <td>September 19, 2012</td>\n",
       "      <td>140716137</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Kh3RD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AB404BB664D653ECF79DE0E0867F6D34</td>\n",
       "      <td>Las Vegas, Nevada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Stay Away”</td>\n",
       "      <td>Found Racist graffiti in the room. Request to ...</td>\n",
       "      <td>June 2011</td>\n",
       "      <td>240151</td>\n",
       "      <td>1</td>\n",
       "      <td>June 27, 2011</td>\n",
       "      <td>114807323</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TheUglyPhotographer</td>\n",
       "      <td>4.0</td>\n",
       "      <td>BB116F87FE8F9AB356F63853BFD32FFE</td>\n",
       "      <td>Oceanside, California</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Great location and service”</td>\n",
       "      <td>Close to Union Square - hotel is a bit of a ma...</td>\n",
       "      <td>October 2010</td>\n",
       "      <td>80793</td>\n",
       "      <td>0</td>\n",
       "      <td>October 25, 2010</td>\n",
       "      <td>84805430</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Moonstonemoclips</td>\n",
       "      <td>48.0</td>\n",
       "      <td>F3D0CF371B788300E73A1413B2DABB4B</td>\n",
       "      <td>Kirkland</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“I will never go back here again!”</td>\n",
       "      <td>I had a bad vibe about this place from the mom...</td>\n",
       "      <td>June 2012</td>\n",
       "      <td>111418</td>\n",
       "      <td>1</td>\n",
       "      <td>June 28, 2012</td>\n",
       "      <td>132971117</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>JoanellenJ</td>\n",
       "      <td>22.0</td>\n",
       "      <td>BC6BC07F81B768F78B6CE17A18762C11</td>\n",
       "      <td>New York</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Ripped off our VISA card after departure”</td>\n",
       "      <td>After we received our \"final\" bill and left th...</td>\n",
       "      <td>January 2012</td>\n",
       "      <td>671150</td>\n",
       "      <td>3</td>\n",
       "      <td>February 4, 2012</td>\n",
       "      <td>124104157</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lynnworks</td>\n",
       "      <td>3.0</td>\n",
       "      <td>F7E9D044FA2554FD06A871289312E043</td>\n",
       "      <td>Providence</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        title  \\\n",
       "0                       “Horrible experience”   \n",
       "1                                 “Stay Away”   \n",
       "2                “Great location and service”   \n",
       "3          “I will never go back here again!”   \n",
       "4  “Ripped off our VISA card after departure”   \n",
       "\n",
       "                                                text     date_stayed  \\\n",
       "0  First of all we got there and they didn't have...  September 2012   \n",
       "1  Found Racist graffiti in the room. Request to ...       June 2011   \n",
       "2  Close to Union Square - hotel is a bit of a ma...    October 2010   \n",
       "3  I had a bad vibe about this place from the mom...       June 2012   \n",
       "4  After we received our \"final\" bill and left th...    January 2012   \n",
       "\n",
       "   offering_id  num_helpful_votes                date         id  via_mobile  \\\n",
       "0        80138                  0  September 19, 2012  140716137       False   \n",
       "1       240151                  1       June 27, 2011  114807323       False   \n",
       "2        80793                  0    October 25, 2010   84805430       False   \n",
       "3       111418                  1       June 28, 2012  132971117       False   \n",
       "4       671150                  3    February 4, 2012  124104157       False   \n",
       "\n",
       "   ratings.service  ratings.cleanliness  ...  ratings.rooms  \\\n",
       "0              1.0                  2.0  ...            1.0   \n",
       "1              1.0                  1.0  ...            NaN   \n",
       "2              4.0                  5.0  ...            4.0   \n",
       "3              3.0                  2.0  ...            1.0   \n",
       "4              NaN                  NaN  ...            NaN   \n",
       "\n",
       "       author.username  author.num_reviews                         author.id  \\\n",
       "0                Kh3RD                 1.0  AB404BB664D653ECF79DE0E0867F6D34   \n",
       "1  TheUglyPhotographer                 4.0  BB116F87FE8F9AB356F63853BFD32FFE   \n",
       "2     Moonstonemoclips                48.0  F3D0CF371B788300E73A1413B2DABB4B   \n",
       "3           JoanellenJ                22.0  BC6BC07F81B768F78B6CE17A18762C11   \n",
       "4            Lynnworks                 3.0  F7E9D044FA2554FD06A871289312E043   \n",
       "\n",
       "         author.location author.num_cities  author.num_helpful_votes  \\\n",
       "0      Las Vegas, Nevada               NaN                       NaN   \n",
       "1  Oceanside, California               3.0                       4.0   \n",
       "2               Kirkland              31.0                      27.0   \n",
       "3               New York              10.0                       9.0   \n",
       "4             Providence               3.0                       7.0   \n",
       "\n",
       "  author.num_type_reviews ratings.check_in_front_desk  \\\n",
       "0                     NaN                         NaN   \n",
       "1                     4.0                         NaN   \n",
       "2                    32.0                         NaN   \n",
       "3                     5.0                         NaN   \n",
       "4                     3.0                         NaN   \n",
       "\n",
       "   ratings.business_service_(e_g_internet_access)  \n",
       "0                                             NaN  \n",
       "1                                             NaN  \n",
       "2                                             NaN  \n",
       "3                                             NaN  \n",
       "4                                             NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e967e07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratings.overall\n",
       "5.0    9825\n",
       "4.0    7720\n",
       "3.0    3287\n",
       "2.0    1611\n",
       "1.0    1557\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_overall_train_df = train_df[[\"title\", \"text\", \"ratings.overall\"]]\n",
    "\n",
    "predict_overall_train_df.head()\n",
    "predict_overall_train_df[\"ratings.overall\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff9df57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_overall_train_df.loc[:,'title'] = [\"\".join(filter(lambda c: str.isalnum(c) or str.isspace(c), char)) for char in predict_overall_train_df.title]\n",
    "predict_overall_train_df.loc[:,'text'] = [\"\".join(filter(lambda c: str.isalnum(c) or str.isspace(c), char)) for char in predict_overall_train_df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2364efb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>ratings.overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Horrible experience</td>\n",
       "      <td>First of all we got there and they didnt have ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stay Away</td>\n",
       "      <td>Found Racist graffiti in the room Request to c...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great location and service</td>\n",
       "      <td>Close to Union Square  hotel is a bit of a maz...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I will never go back here again</td>\n",
       "      <td>I had a bad vibe about this place from the mom...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ripped off our VISA card after departure</td>\n",
       "      <td>After we received our final bill and left the ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title  \\\n",
       "0                       Horrible experience   \n",
       "1                                 Stay Away   \n",
       "2                Great location and service   \n",
       "3           I will never go back here again   \n",
       "4  Ripped off our VISA card after departure   \n",
       "\n",
       "                                                text  ratings.overall  \n",
       "0  First of all we got there and they didnt have ...              1.0  \n",
       "1  Found Racist graffiti in the room Request to c...              1.0  \n",
       "2  Close to Union Square  hotel is a bit of a maz...              4.0  \n",
       "3  I had a bad vibe about this place from the mom...              2.0  \n",
       "4  After we received our final bill and left the ...              1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_overall_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb99b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_english_reviews(df):\n",
    "    try:\n",
    "        title_languages = detect_language(df[\"title\"].tolist())\n",
    "        text_languages = detect_language(df[\"text\"].tolist())\n",
    "        return df[(title_languages == \"en\") & (text_languages == \"en\")]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during language detection: {e}\")\n",
    "        return df[[]]  # Return an empty DataFrame if detection fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_overall_train_df = filter_english_reviews(predict_overall_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_overall_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.info())\n",
    "print(predict_overall_train_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d75afc",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ce118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot distribution of ratings\n",
    "train_df['ratings.overall'].value_counts().sort_index().plot(kind='bar', figsize=(8,5), color='skyblue')\n",
    "\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684bc013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)  # Remove common words\n",
    "X = vectorizer.fit_transform(train_df[\"text\"])\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "correlations = tfidf_df.corrwith(train_df[\"ratings.overall\"])\n",
    "correlations = correlations.sort_values(ascending=False)  # Sort by importance\n",
    "\n",
    "# Find words with the weakest correlation (near zero)\n",
    "non_predictive_words = correlations.sort_values(key=lambda x: np.abs(x))\n",
    "print(\"Non-Predictive Words:\\n\", non_predictive_words.head(10))\n",
    "\n",
    "# Display top positive and negative correlated words\n",
    "print(\"Most Positive Words:\\n\", correlations.head(10))\n",
    "print(\"\\nMost Negative Words:\\n\", correlations.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726be157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split all reviews into words and find unique ones\n",
    "all_words_text = predict_overall_train_df.text.apply(nltk.word_tokenize)\n",
    "all_words_title = predict_overall_train_df.title.apply(nltk.word_tokenize)\n",
    "\n",
    "all_words = pd.concat([all_words_text, all_words_title])\n",
    "unique_words = np.unique(all_words)\n",
    "\n",
    "print(\"Total Unique Words:\", len(unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency plots are used to visualize the most common words in a dataset.\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = [\n",
    "    word.lower() for sentence in unique_words.tolist()\n",
    "    for s in sentence\n",
    "    for word in word_tokenize(s)\n",
    "    if word.isalnum() and word.lower() not in stop_words\n",
    "]\n",
    "\n",
    "word_freq = Counter(tokens)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(*zip(*word_freq.most_common(20)))\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Word Frequency Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying common N-grams: N-grams are sequences of words. \n",
    "# Identifying common n-grams can give insight into the most common phrases in the dataset.\n",
    "\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to generate n-grams\n",
    "def generate_ngrams(text, n):\n",
    "    n_grams = ngrams(text, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "# Specify the value of n for n-grams\n",
    "n_value = 3  # You can change this value to see different n-grams, e.g., 2 for bigrams, 3 for trigrams, etc.\n",
    "\n",
    "# Generate n-grams\n",
    "ngrams_list = generate_ngrams(tokens, n_value)\n",
    "\n",
    "# Count the occurrences of each n-gram\n",
    "ngrams_count = Counter(ngrams_list)\n",
    "\n",
    "# Display the distribution\n",
    "print(f\"Distribution of {n_value}-grams:\")\n",
    "for ngram, count in ngrams_count.items():\n",
    "    print(f\"{ngram}: {count}\")\n",
    "\n",
    "# Plot the distribution\n",
    "labels, values = zip(*ngrams_count.items())\n",
    "indexes = range(len(labels))\n",
    "\n",
    "plt.bar(indexes, values)\n",
    "plt.xlabel(f'{n_value}-grams')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(indexes, labels, rotation='vertical')\n",
    "plt.title(f'Distribution of {n_value}-grams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333cefa8",
   "metadata": {},
   "source": [
    "### 3. Selection and training Machine Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26cb94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_res = pd.concat([predict_overall_train_df[\"title\"], predict_overall_train_df[\"text\"]], axis=1)\n",
    "X_res = X_res[\"title\"] + \" \" + X_res[\"text\"]\n",
    "y_res = predict_overall_train_df[\"ratings.overall\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d73fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1ab6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_train = {}\n",
    "result_dict_test = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db783c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f580e54",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e89d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# https://www.geeksforgeeks.org/multinomial-naive-bayes/\n",
    "\n",
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9755c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_accuracies = cross_val_score(classifier, X_train_vectors, y_train, cv=5)\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "print(\"Naive Bayes Train Score:\", np.mean(nb_accuracies))\n",
    "result_dict_train[\"Naive Bayes Default Train Score\"] = np.mean(nb_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_score = classifier.score(X_test_vectors, y_test)\n",
    "print(\"Naive Bayes Test Score:\", naive_bayes_score)\n",
    "result_dict_test[\"Naive Bayes Default Test Score\"] = naive_bayes_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8697af15",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ceba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_accuracies = cross_val_score(classifier, X_train_vectors, y_train, cv=5)\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "print(\"SVM Train Score:\", np.mean(svc_accuracies))\n",
    "result_dict_train[\"SVM Default Train Score\"] = np.mean(svc_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_score = classifier.score(X_test_vectors, y_test)\n",
    "print(\"SVM Test Score:\", svc_score)\n",
    "result_dict_test[\"SVM Default Test Score\"] = svc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c1c6567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 18:11:13.891310: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-28 18:11:13.900671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743147673.910271   12107 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743147673.912904   12107 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743147673.919816   12107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743147673.919832   12107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743147673.919833   12107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743147673.919834   12107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-28 18:11:13.922332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743147675.262086   12107 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6144 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Limit GPU memory usage\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.LogicalDeviceConfiguration(memory_limit=(6 * 1024))])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8403817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743147680.644927   12200 service.cc:152] XLA service 0x7820fc0029e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1743147680.644942   12200 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050, Compute Capability 8.6\n",
      "2025-03-28 18:11:20.659046: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1743147680.721697   12200 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 43/625\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3639 - loss: 1.5294"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743147681.725449   12200 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.4770 - loss: 1.2252 - val_accuracy: 0.5962 - val_loss: 0.9336\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6856 - loss: 0.7768 - val_accuracy: 0.6037 - val_loss: 0.9476\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7841 - loss: 0.5499 - val_accuracy: 0.5975 - val_loss: 0.9676\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8727 - loss: 0.3721 - val_accuracy: 0.5885 - val_loss: 1.0662\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9182 - loss: 0.2538 - val_accuracy: 0.5927 - val_loss: 1.0762\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9466 - loss: 0.1871 - val_accuracy: 0.5879 - val_loss: 1.2293\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9580 - loss: 0.1472 - val_accuracy: 0.5750 - val_loss: 1.2839\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9692 - loss: 0.1101 - val_accuracy: 0.5821 - val_loss: 1.3997\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9715 - loss: 0.0951 - val_accuracy: 0.5700 - val_loss: 1.5339\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9737 - loss: 0.0864 - val_accuracy: 0.5790 - val_loss: 1.5401\n"
     ]
    }
   ],
   "source": [
    "subset_size = 5000 # X_train_vectors.shape[0]\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_adjusted = y_train - 1\n",
    "y_test_adjusted = y_test - 1  \n",
    "\n",
    "y_train_cat = to_categorical(y_train_adjusted, num_classes=5)\n",
    "y_test_cat = to_categorical(y_test_adjusted, num_classes=5)\n",
    "\n",
    "X_train_subset = X_train_vectors[:subset_size]\n",
    "y_train_subset = y_train_cat[:subset_size]\n",
    "X_val_subset = X_test_vectors[:subset_size]\n",
    "y_val_subset = y_test_cat[:subset_size]\n",
    "\n",
    "X_train_tfidf = X_train_subset.toarray()\n",
    "X_val_tfidf = X_val_subset.toarray()\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(X_train_tfidf.shape[1],)),  # Input layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Hidden layer\n",
    "    tf.keras.layers.Dropout(0.5),  # Dropout for regularization\n",
    "    tf.keras.layers.Dense(5, activation='softmax')  # Output layer for 5 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tfidf, y_train_subset)).batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_tfidf, y_val_subset)).batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6843d793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 17:38:29.672884: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.76GiB (rounded to 7263744000)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-03-28 17:38:29.672919: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1058] BFCAllocator dump for GPU_0_bfc\n",
      "2025-03-28 17:38:29.672930: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (256): \tTotal Chunks: 18, Chunks in use: 18. 4.5KiB allocated for chunks. 4.5KiB in use in bin. 608B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.672937: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (512): \tTotal Chunks: 1, Chunks in use: 1. 512B allocated for chunks. 512B in use in bin. 512B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.672944: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2025-03-28 17:38:29.672949: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.672955: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.672973: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.672978: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.672987: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (32768): \tTotal Chunks: 2, Chunks in use: 1. 94.5KiB allocated for chunks. 32.0KiB in use in bin. 32.0KiB client-requested in use in bin.\n",
      "2025-03-28 17:38:29.672992: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.672997: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.673002: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.673007: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.673012: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.673017: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.673022: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.673027: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.673032: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.673041: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.673046: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.673051: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-03-28 17:38:29.673059: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 1. 4.10GiB allocated for chunks. 894.57MiB in use in bin. 894.57MiB client-requested in use in bin.\n",
      "2025-03-28 17:38:29.673065: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1081] Bin for 6.76GiB was 256.00MiB, Chunk State: \n",
      "2025-03-28 17:38:29.673077: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1087]   Size: 1.48GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 32.0KiB | Requested Size: 32.0KiB | in_use: 1 | bin_num: -1\n",
      "2025-03-28 17:38:29.673088: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1087]   Size: 1.75GiB | Requested Size: 8B | in_use: 0 | bin_num: 20, prev:   Size: 256B | Requested Size: 4B | in_use: 1 | bin_num: -1, next:   Size: 894.57MiB | Requested Size: 894.57MiB | in_use: 1 | bin_num: -1\n",
      "2025-03-28 17:38:29.673093: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1094] Next region of size 4399824896\n",
      "2025-03-28 17:38:29.673101: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bce8000000 of size 1280 next 1\n",
      "2025-03-28 17:38:29.673105: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bce8000500 of size 256 next 2\n",
      "2025-03-28 17:38:29.673109: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bce8000600 of size 256 next 3\n",
      "2025-03-28 17:38:29.673113: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bce8000700 of size 256 next 4\n",
      "2025-03-28 17:38:29.673117: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bce8000800 of size 256 next 5\n",
      "2025-03-28 17:38:29.673121: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bce8000900 of size 256 next 6\n",
      "2025-03-28 17:38:29.673126: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] Free  at 78bce8000a00 of size 1876057088 next 12\n",
      "2025-03-28 17:38:29.673130: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd57d26600 of size 938028032 next 13\n",
      "2025-03-28 17:38:29.673135: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9200 of size 256 next 7\n",
      "2025-03-28 17:38:29.673140: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9300 of size 512 next 9\n",
      "2025-03-28 17:38:29.673145: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9500 of size 256 next 10\n",
      "2025-03-28 17:38:29.673149: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9600 of size 256 next 8\n",
      "2025-03-28 17:38:29.673154: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9700 of size 256 next 11\n",
      "2025-03-28 17:38:29.673158: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9800 of size 256 next 15\n",
      "2025-03-28 17:38:29.673162: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9900 of size 256 next 16\n",
      "2025-03-28 17:38:29.673166: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9a00 of size 256 next 14\n",
      "2025-03-28 17:38:29.673171: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9b00 of size 256 next 20\n",
      "2025-03-28 17:38:29.673178: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9c00 of size 256 next 17\n",
      "2025-03-28 17:38:29.673182: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9d00 of size 256 next 21\n",
      "2025-03-28 17:38:29.673186: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9e00 of size 256 next 22\n",
      "2025-03-28 17:38:29.673190: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbb9f00 of size 256 next 23\n",
      "2025-03-28 17:38:29.673194: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbba000 of size 256 next 24\n",
      "2025-03-28 17:38:29.673198: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] Free  at 78bd8fbba100 of size 64000 next 18\n",
      "2025-03-28 17:38:29.673203: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78bd8fbc9b00 of size 32768 next 19\n",
      "2025-03-28 17:38:29.673207: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] Free  at 78bd8fbd1b00 of size 1585636608 next 18446744073709551615\n",
      "2025-03-28 17:38:29.673212: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1119]      Summary of in-use Chunks by size: \n",
      "2025-03-28 17:38:29.673219: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 18 Chunks of size 256 totalling 4.5KiB\n",
      "2025-03-28 17:38:29.673224: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 1 Chunks of size 512 totalling 512B\n",
      "2025-03-28 17:38:29.673229: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2025-03-28 17:38:29.673234: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 1 Chunks of size 32768 totalling 32.0KiB\n",
      "2025-03-28 17:38:29.673239: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 1 Chunks of size 938028032 totalling 894.57MiB\n",
      "2025-03-28 17:38:29.673245: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1126] Sum Total of in-use chunks: 894.61MiB\n",
      "2025-03-28 17:38:29.673250: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1128] Total bytes in pool: 4399824896 memory_limit_: 4399824896 available bytes: 0 curr_region_allocation_bytes_: 8799649792\n",
      "2025-03-28 17:38:29.673261: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1133] Stats: \n",
      "Limit:                      4399824896\n",
      "InUse:                       938067200\n",
      "MaxInUse:                   2814087424\n",
      "NumAllocs:                          51\n",
      "MaxAllocSize:                938028032\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-03-28 17:38:29.673268: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:512] *_________________________________________**********************____________________________________\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4627/1113805533.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create TensorFlow datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_TensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    132\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 134\u001b[0;31m               \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"component_%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    734\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    737\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_tensor_conversion.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    277\u001b[0m                         \u001b[0mallow_broadcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m   const_tensor = ops._create_graph_constant(  # pylint: disable=protected-access\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_EagerTensorBase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "# Convert sparse matrices to dense matrices\n",
    "X_train_tfidf = X_train_vectors.toarray()\n",
    "X_val_tfidf = X_test_vectors.toarray()\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tfidf, y_train)).batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_tfidf, y_test)).batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6618b5",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fd58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, optimizers, models\n",
    "\n",
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "\n",
    "def create_rnn_lstm():\n",
    "    model = layers.Sequential()\n",
    "    model.add(layers.Embedding(vocabulary_size, 25, input_length=seq_len))\n",
    "    model.add(layers.LSTM(150, return_sequences=True))\n",
    "    model.add(layers.LSTM(150))\n",
    "    model.add(layers.Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "\n",
    "classifier.fit(X_train_vectors, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_dict_train)\n",
    "print(result_dict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067cebcf",
   "metadata": {},
   "source": [
    "### 4. Experiment with VADER sentiment lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c816a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "333ba5cb",
   "metadata": {},
   "source": [
    "### 5. Final testing on test set and discussion of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6ebe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba7ea3a2",
   "metadata": {},
   "source": [
    "### 6. Propose a method to predict aspects \n",
    "\n",
    "***(COMP SCI 7417 and COMP SCI 7717 only)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cf2087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23d3d17f",
   "metadata": {},
   "source": [
    "### 7. Reflection on the ***Product*** development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4dae7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af38d9fc",
   "metadata": {},
   "source": [
    "### 9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3a062",
   "metadata": {},
   "source": [
    "https://pandas.pydata.org/pandas-docs/version/1.2.0/reference/api/pandas.json_normalize.html\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2022/07/step-by-step-exploratory-data-analysis-eda-using-python/\n",
    "\n",
    "https://pawarbi.github.io/blog/pandas/numpy/data-cleaning/2021/03/05/removing-non-alphanumeric-symbols-characters-from-column-numpy-pandas-dataframe.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c088010b-92f5-499b-a24e-4e3f874f4950",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7289b0-b12b-4be4-bc07-0b41a55266c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
