{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop topic:Text Generation, Summarisation and Prompting\n",
    "\n",
    "### Introduction to Text Generation using GPT-2\n",
    "\n",
    "Text generation stands as one of the most useful applications of Natural Language Processing.\n",
    "Decode based GPT is the still the SOTA of text generation and other NLP tasks. \n",
    "Although we cannot use the latest GPT-4, we are going to use GPT-2, which is a smaller pre-trained model that can be run with limited resources.\n",
    "\n",
    "## Activity 1: GPT-2 Open Text Generation\n",
    "\n",
    "Work on this activity is groups (one at each table)\n",
    "\n",
    "1. Review the following code to understand its working\n",
    "2. Think of a few more prompting examples, and generate texts using them\n",
    "3. Open current ChatGPT and use the same examples to generate texts\n",
    "4. Compare GPT-2 with current ChatGPT generation applying human evaluation criteria discussed in Lecture 11. Apply scoring from 1 to 5 for each criteria, add scored together to comare the models.\n",
    "    fluency\n",
    "    coherence / consistency\n",
    "    factuality and correctness\n",
    "    commonsense\n",
    "    style / formality\n",
    "    grammaticality\n",
    "    typicality (what type of something, exemplars etc.)\n",
    "    redundancy\n",
    "5. Discuss your findings in the class. What are the variations between different groups in the class in evaluating texts?\n",
    "\n",
    "**Explanation of code:**\n",
    "\n",
    "    Tokenizer Initialization: The code initializes a GPT-2 tokenizer (tokenizer) to preprocess text inputs. Tokenizers break down input text into tokens, which are numerical representations used by the model.\n",
    "\n",
    "    Model Initialization: The GPT-2 model (model) is loaded. This model is a pre-trained neural network that has learned to predict the next word in a sequence given some context.\n",
    "\n",
    "    Maximum Length: max_length is set to control the length of the generated text. This prevents the model from generating excessively long responses.\n",
    "\n",
    "    Input Prompt: The prompt variable contains the initial snippet of text provided to the model for text generation.\n",
    "\n",
    "    Encoding the Input: The encode() method of the tokenizer converts the input prompt into token IDs (input_ids). These token IDs are the numerical representations of the input text.\n",
    "\n",
    "    Text Generation: The generate() method of the GPT-2 model generates text based on the input token IDs (input_ids). The do_sample=True parameter allows for sampling from the model's predicted probability distribution, adding randomness to the generated text.\n",
    "\n",
    "    Decoding the Output: The decode() method of the tokenizer converts the generated token IDs (output_ids) back into text, excluding any special tokens such as padding or separator tokens.\n",
    "\n",
    "    Printing the Output: The generated text (output_text) is printed to the console for visualization.\n",
    "\n",
    "This code demonstrates the process of using GPT-2 for text generation based on an initial prompt, providing participants with a hands-on understanding of how the model operates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the maximum length of the generated text\n",
    "max_length = 100\n",
    "\n",
    "# Define the input prompt\n",
    "prompt = \"The quick brown fox\"\n",
    "\n",
    "# Encode the input prompt using the tokenizer\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate the text using the GPT-2 model\n",
    "output_ids = model.generate(input_ids=input_ids, max_length=max_length, do_sample=True)\n",
    "\n",
    "# Decode the generated text using the tokenizer\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2: ChatGPT Prompting\n",
    "\n",
    "Work on this activity in groups (one at each table)\n",
    "\n",
    "### Example of Few Shot Prompting\n",
    "\n",
    "1. Try this example using zero-shot pronpting first. Your prompt would be:\n",
    "\"Convert the following plain English sentence into formal legal language: If you break the rules, you might get kicked out.\"\n",
    "2. Note the response.\n",
    "3. Now enter the following few-shot prompting and compare the response with the zero-shot. Did you get better response?\n",
    "\n",
    "\n",
    "-------- prompt -----------------\n",
    "\n",
    "Convert the following plain English sentences into formal legal language.\n",
    "\n",
    "**Example 1:**  \n",
    "**Input:** You must return the rental car by 5 PM.  \n",
    "**Output:** The renter shall return the vehicle no later than 5:00 PM on the agreed-upon date.\n",
    "\n",
    "**Example 2:**  \n",
    "**Input:** We can cancel the contract if you don't pay on time.  \n",
    "**Output:** The agreement may be terminated by the first party in the event of a failure by the second party to render payment in a timely manner.\n",
    "\n",
    "**Example 3:**  \n",
    "**Input:** You're not allowed to share this document with anyone.  \n",
    "**Output:** Disclosure of this document to any third party is strictly prohibited.\n",
    "\n",
    "**Now your turn:**  \n",
    "**Input:** If you break the rules, you might get kicked out.  \n",
    "**Output:**\n",
    "\n",
    "----------- end of prompt ----------------------\n",
    "\n",
    "4. Now think of a harder example that zero-shot might have trouble with. Try it and note results.\n",
    "\n",
    "Note: Maybe for these relatively simple examples it would not make make much difference, but you got the idea how it works.\n",
    "\n",
    "\n",
    "### Example of Chain-of-Thought Prompting\n",
    "\n",
    "CoT prompting us used for complex reasoning tasks, for example, tasks with many constraints.\n",
    "\n",
    "Here is an example of CoT prompting to solve tasks with contraints. \n",
    "1. First, enter just the last task (scroll down) as zero-shot starting with \"Here is the task for you:\". Note the result. Was the task solved correctly?\n",
    "2. Next enther the following CoT prompt and again, note the result. Did ChatGPT solve your task correctly? Did it follow your example closely? \n",
    "3. Think of another example of possibly harder task (maybe 4 constraints) and follow points 1 and 2.\n",
    "\n",
    "------------- prompt starts here ----------------\n",
    "\n",
    "\n",
    "**Question:**  \n",
    "Three friends — Alice, Bob, and Carol — are sitting in a row of three chairs.  \n",
    "- Alice does not want to sit next to Bob.  \n",
    "- Bob must sit to the left of Carol.  \n",
    "- Carol is not sitting in the middle.\n",
    "\n",
    "Where is everyone sitting?\n",
    "\n",
    "Let's think step by step:\n",
    "\n",
    "1. First, let's list all the possible permutations of the three people:  \n",
    "   ABC, ACB, BAC, BCA, CAB, CBA.\n",
    "\n",
    "2. Now apply the constraint: **Bob must sit to the left of Carol.**  \n",
    "   \n",
    "   For Bob to be to the left of Carol, he must appear **before** Carol in the order.\n",
    "\n",
    "   Let’s check each:\n",
    "   - **ABC** → B is before C (valid)  \n",
    "   - **ACB** → B is after C (invalid  \n",
    "   - **BAC** → B is before C (valid)\n",
    "   - **BCA** → B is before C (valid)\n",
    "   - **CAB** → B is after C (invalid)\n",
    "   - **CBA** → B is after C (invalid) \n",
    "\n",
    "   Valid ones: ABC, BAC, BCA\n",
    "\n",
    "3. Next constraint: **Carol is not in the middle.**\n",
    "\n",
    "   From our remaining options:\n",
    "   - **ABC** → Carol is in position 3 (valid)  \n",
    "   - **BAC** → Carol is in position 3 (valid) \n",
    "   - **BCA** → Carol is in position 2 (invalid) \n",
    "\n",
    "   BCA is eliminated. Remaining: ABC, BAC\n",
    "\n",
    "4. Now apply: **Alice does not want to sit next to Bob.**\n",
    "\n",
    "   - **ABC** → Alice is position 1, Bob is 2 → next to each other (invalid)   \n",
    "   - **BAC** → Bob is 1, Alice is 2 → also next to each other (invalid) \n",
    "\n",
    "Let’s re-express the constraints more carefully:\n",
    "- Alice ≠ adjacent to Bob  \n",
    "- Bob < Carol in position  \n",
    "- Carol ≠ middle seat\n",
    "\n",
    "Try all permutations again but check **all constraints** one by one:\n",
    "\n",
    "| Permutation | Bob < Carol? | Carol ≠ Middle? | Alice ≠ Next to Bob? | Valid? |\n",
    "|-------------|---------------|------------------|------------------------|----------|\n",
    "| ABC         | Yes           | Yes (Carol is 3) | No (Alice next to Bob) | (invalid) |\n",
    "| ACB         | No            | No (Carol is 2)  | Yes                    | (invalid) |\n",
    "| BAC         | Yes           | Yes (Carol is 3) | No                     | (invalid) |\n",
    "| BCA         | Yes           | No (Carol is 2)  | Yes                    | (invalid) |\n",
    "| CAB         | No            | Yes (Carol is 1) | Yes                    | (invalid) |\n",
    "| CBA         | No            | Yes (Carol is 1) | No                     | (invalid) |\n",
    "\n",
    "All are invalid!  \n",
    "So the conclusion is:\n",
    "\n",
    "There is no valid seating arrangement that satisfies all three constraints.\n",
    "\n",
    "Now your turn.\n",
    "\n",
    "Here is the task for you: \n",
    "\n",
    "I have to schedule three meetings in one day: Meeting A, meeting B and meeting C.\n",
    "Meeting A cannot happen before meeting B\n",
    "Meeting C cannot be the first\n",
    "Meeting B Cannot be the last\n",
    "Meetings A and B cannot be scheduled next to each other.\n",
    "\n",
    "Find all possible schedules of these meetings. \n",
    "\n",
    "\n",
    "------------ end of prompt ----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3: BART text Summarisation\n",
    "\n",
    "The following code can summarise text using Bart and trasformer pipeline.\n",
    "\n",
    "1. Review the example code below\n",
    "2. In the **second cell of code**, implement article summarisation, both abstractive and extractive, from given short news. \n",
    "3. Compare these two types of summarisation using ROUGE, as well as human evaluation as in Activity 2.\n",
    "4. Answer the following questions:\n",
    "    \n",
    "    a. Which type of summarisation generally gives better ROUGE score?\n",
    "    \n",
    "    b. Which type of summarisation generally gives better human score?\n",
    "    \n",
    " Discuss the results in the class. If you find these articles hard to assess the quality of summarisation, you can use some articles from your assignment 2, but need to provide a reference summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summarisation example\n",
    "\n",
    "!pip install rouge\n",
    "\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load the BART tokenizer and model for abstractive summarization\n",
    "tokenizer_abstractive = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model_abstractive = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Load the pipeline for extractive summarization\n",
    "pipeline_extractive = pipeline('summarization')\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"The quick brown fox jumps over the lazy dog. This is a test sentence for summarization. Here is another sentence for testing.\"\n",
    "\n",
    "# Define the target summary\n",
    "target_summary = \"The quick brown fox jumps over the lazy dog. This is a test sentence for summarization.\"\n",
    "\n",
    "# Perform abstractive summarization using BART\n",
    "inputs = tokenizer_abstractive([input_text], max_length=1024, truncation=True, padding='max_length', return_tensors='pt')\n",
    "outputs = model_abstractive.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=60, num_beams=4, length_penalty=2.0)\n",
    "summary_abstractive = tokenizer_abstractive.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Perform extractive summarization using pipeline\n",
    "summary_extractive = pipeline_extractive(input_text, max_length=60)[0]['summary_text']\n",
    "\n",
    "# Evaluate the summaries using the ROUGE metric\n",
    "rouge = Rouge()\n",
    "scores_abstractive = rouge.get_scores(summary_abstractive, target_summary)\n",
    "scores_extractive = rouge.get_scores(summary_extractive, target_summary)\n",
    "\n",
    "# Print the summaries and ROUGE scores\n",
    "print(\"Input Text: \", input_text)\n",
    "print(\"Target Summary: \", target_summary)\n",
    "print(\"Abstractive Summary: \", summary_abstractive)\n",
    "print(\"ROUGE Scores for Abstractive Summary: \", scores_abstractive)\n",
    "print(\"Extractive Summary: \", summary_extractive)\n",
    "print(\"ROUGE Scores for Extractive Summary: \", scores_extractive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summarisation example\n",
    "\n",
    "# !pip install rouge\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load the CNN/DailyMail dataset\n",
    "df = pd.read_csv('./daily_cnn.csv')\n",
    "\n",
    "for all_articles_in_file:\n",
    "    ...\n",
    "    # Print the summaries and ROUGE scores\n",
    "    print(\"Input Text: \", input_text)\n",
    "    print(\"Target Summary: \", target_summary)\n",
    "    print(\"Abstractive Summary: \", summary_abstractive)\n",
    "    print(\"ROUGE Scores for Abstractive Summary: \", scores_abstractive)\n",
    "    print(\"Extractive Summary: \", summary_extractive)\n",
    "    print(\"ROUGE Scores for Extractive Summary: \", scores_extractive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4 (optional, if time allows): BART text Summarisation\n",
    "\n",
    "Compare both summarisations of Daily CNN news. \n",
    "\n",
    "In the class, discuss the following question: Which summarisation type might be suitable for what type of application?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
