{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Introduction to NLP</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. NLP (Natural Language Processing) is a field of computer science and artificial intelligence that deals with the interaction between computers and human languages.\n",
    "2. Common use cases of NLP include text classification, sentiment analysis, language translation, and speech recognition.\n",
    "3. Important concepts in NLP include tokenization, stemming, stop words, and n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Review the below code\n",
    "\n",
    "Please review the following code and answer associated questions.\n",
    "\n",
    "You will use this or similar code in the exercise that follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Cleaning and Preprocessing: Techniques for preparing text data for analysis, including tokenization, stemming, and stop word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Tokenization is the process of breaking down a sentence into individual words or phrases.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# nltk.download('punkt')\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Tokenization is the process of breaking down a sentence into individual words or phrases.\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"NLP is a field of computer science, and artificial intelligence\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing: Converting all text to lowercase can make it easier to analyze the text and remove case-sensitive duplicates.\n",
    "text = \"NLP Is a Field of Computer Science, and Artificial Intelligence\"\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation: Punctuation marks can add noise to the text and make it harder to analyze. Removing them can make the text easier to work with.\n",
    "\n",
    "import string\n",
    "\n",
    "text = \"NLP Is a Field of Computer Science, and Artificial Intelligence!\"\n",
    "text = text.translate(text.maketrans('', '', string.punctuation))\n",
    "print(text)\n",
    "# Output: \"NLP Is a Field of Computer Science and Artificial Intelligence\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words are common words that are typically removed from text data before analysis.\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words = [\"NLP\", \"is\", \"a\", \"field\", \"of\", \"computer\", \"science\", \"and\", \"artificial\", \"intelligence\"]\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming is the process of reducing a word to its root form.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define a sample text\n",
    "text = \"The quick brown foxes are better than lazy dogs.\"\n",
    "tokens = text.split()\n",
    "\n",
    "stemmed_text = ' '.join([stemmer.stem(token) for token in tokens])\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Stemmed Text:\", stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Lematization\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    " \n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    " \n",
    "# Define a sample text\n",
    "text = \"The quick brown foxes are better than lazy dogs.\"\n",
    " \n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    " \n",
    "# Extract lemmatized tokens\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n",
    " \n",
    "# Join the lemmatized tokens into a sentence\n",
    "lemmatized_text = ' '.join(lemmatized_tokens)\n",
    " \n",
    "# Print the original and lemmatized text\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Lemmatized Text:\", lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer the following questions**: \n",
    "1. How stemming and lemmatisation results are different?\n",
    "2. What are other differences between stemming and lemmatisation?\n",
    "3. Think of a case when you would use stemming and when you would use lemmatisation.\n",
    "\n",
    "You can search external materials to answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis (EDA) is an important step in any NLP project. It helps to understand the characteristics of the data and identify any potential issues or patterns that may be relevant to the analysis. Here are some simple examples of EDA techniques that can be applied to text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency plots are used to visualize the most common words in a dataset.\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = \"NLP is a field of computer science and artificial intelligence. NLP is a field of study that focuses on how computers can understand, interpret and generate human language\"\n",
    "tokens = word_tokenize(text)\n",
    "filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "word_freq = Counter(filtered_words)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.bar(word_freq.keys(), word_freq.values())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identifying common N-grams: N-grams are sequences of words. \n",
    "# Identifying common n-grams can give insight into the most common phrases in the dataset.\n",
    "\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample text\n",
    "sample_text = \"NLP is a field of computer science and artificial intelligence. NLP is a field of study that focuses on how computers can understand, interpret and generate human language\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = nltk.word_tokenize(sample_text)\n",
    "\n",
    "# Function to generate n-grams\n",
    "def generate_ngrams(text, n):\n",
    "    n_grams = ngrams(text, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "# Specify the value of n for n-grams\n",
    "n_value = 3  # You can change this value to see different n-grams, e.g., 2 for bigrams, 3 for trigrams, etc.\n",
    "\n",
    "# Generate n-grams\n",
    "ngrams_list = generate_ngrams(tokens, n_value)\n",
    "\n",
    "# Count the occurrences of each n-gram\n",
    "ngrams_count = Counter(ngrams_list)\n",
    "\n",
    "# Display the distribution\n",
    "print(f\"Distribution of {n_value}-grams:\")\n",
    "for ngram, count in ngrams_count.items():\n",
    "    print(f\"{ngram}: {count}\")\n",
    "\n",
    "# Plot the distribution\n",
    "labels, values = zip(*ngrams_count.items())\n",
    "indexes = range(len(labels))\n",
    "\n",
    "plt.bar(indexes, values)\n",
    "plt.xlabel(f'{n_value}-grams')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(indexes, labels, rotation='vertical')\n",
    "plt.title(f'Distribution of {n_value}-grams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Thre are more EDA method, apart from word and n-grm frequencies. For example, distribution of instances over classes, and other methods that you know already from Machine Learning. It also may be useful to see TFIDF of words. TFIDF score can tell us importance of words in documents. Hi TFIDF for a document indicates that the term is important for that document. Terms with low TFIDF in all documents can be considered redundunt.\n",
    "\n",
    "**Question**:\n",
    "\n",
    "Can words with high frequency be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "Feature engineering is the process of creating useful features from raw text data that can be used as inputs for NLP models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words: Bag-of-Words (BoW) is a representation of text as a bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. \n",
    "# It is one of the most common feature engineering techniques used in NLP.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"NLP is a field of computer science and artificial intelligence\",\n",
    "          \"NLP is a field of study that focuses on how computers can understand, interpret and generate human language\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(corpus)\n",
    "print(bow.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency): This is a technique that weighs the words based on their importance in the document. \n",
    "# It is a way to convert the raw word counts into meaningful values by multiplying the word count with the inverse of the number of documents where the word is present.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"NLP is a field of computer science and artificial intelligence\",\n",
    "          \"NLP is a field of study that focuses on how computers can understand, interpret and generate human language\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "print(tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "Based on these results, identify three words that you think can be redundant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  N-Grams are sequences of words. Identifying common n-grams can give insight into the most common phrases in the dataset.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"NLP is a field of computer science and artificial intelligence\",\n",
    "          \"NLP is a field of study that focuses on how computers can understand, interpret and generate human language\"]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "ngrams = vectorizer.fit_transform(corpus)\n",
    "print(ngrams.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embeddings are dense vector representations of words. \n",
    "# They capture the meaning and context of words in a way that allows them to be used as input for machine learning models.\n",
    "# !pip install gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"NLP\", \"is\", \"a\", \"field\", \"of\", \"computer\", \"science\", \"and\", \"artificial\", \"intelligence\"],\n",
    "             [\"NLP\", \"is\", \"a\", \"field\", \"of\", \"study\", \"that\", \"focuses\", \"on\", \"how\", \"computers\", \"can\", \"understand\", \"interpret\", \"and\", \"generate\", \"human\", \"language\"]]\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "print(model.wv[\"NLP\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling with supervised models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "These models are trained using labeled data, where the target variable (or output) is known. \n",
    "Common supervised models include text classification and Name Entity Recognition (NER).\n",
    "\n",
    "Text classification: This is a model that is trained to classify text into one or more predefined categories. Example: classifying movie reviews as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text Classification Example\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# dataset\n",
    "X_train = [\"I watched a good movie\", \"This movie is terrible\", \"The movie was average\", \"I liked the movie\"]\n",
    "y_train = [\"positive\", \"negative\", \"neutral\", \"positive\"]\n",
    "X_test = [\"The movie was good\", \"I did not like the movie\"]\n",
    "y_test = [\"positive\", \"negative\"]\n",
    "\n",
    "# feature extraction\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# model training\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# model prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "print(y_pred)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that a sentence with \"like\" and \"did not like\" are both positive!**\n",
    "\n",
    "What can be done to classify this sentence as negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentimental Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis is a field of Natural Language Processing (NLP) that deals with determining the sentiment or emotion expressed in a piece of text. The task involves classifying a given text as positive, negative or neutral, based on the sentiment it conveys.\n",
    "\n",
    "For example, consider a toy dataset with a few comments or statements about a product:\n",
    "\n",
    "    \"This product is amazing! I highly recommend it.\"\n",
    "    \"I was really disappointed with this product.\"\n",
    "    \"It's okay, but I've had better.\"\n",
    "    \"I can't say if it's good or bad, it's just average.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sentiment analysis model can be trained on this dataset to predict the sentiment of each statement. The first statement would be classified as positive, the second as negative, the third as neutral, and the fourth as neutral.\n",
    "\n",
    "The model is trained using labeled data and machine learning algorithms, such as logistic regression or recurrent neural networks. These algorithms learn patterns in the data and use that information to make predictions on new, unseen data.\n",
    "\n",
    "Sentiment Analysis has a wide range of applications, such as in customer feedback analysis, market research, and social media monitoring. It can help businesses understand their customers' opinions, track brand reputation, and monitor the public's response to events.\n",
    "\n",
    "\n",
    "Here is an example of sentiment classification using VADER lexicon and utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Downloading required resources\n",
    "# nltk.download(\"vader_lexicon\")\n",
    "\n",
    "# Initializing the SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Defining a function to get the sentiment score\n",
    "def get_sentiment(text):\n",
    "    sentiment_score = sia.polarity_scores(text)\n",
    "    return sentiment_score[\"compound\"]\n",
    "\n",
    "# Testing the model on a sample text\n",
    "text = \"This product is amazing! I highly recommend it.\"\n",
    "sentiment = get_sentiment(text)\n",
    "if sentiment > 0:\n",
    "    print(\"Positive\")\n",
    "elif sentiment == 0:\n",
    "    print(\"Neutral\")\n",
    "else:\n",
    "    print(\"Negative\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, VADER does not need training, it is an unsupervised method, and it can classify a sentence quite well.\n",
    "\n",
    "But is is better than more complex supervised classifiers? You will answer this question in your Assignment 1.\n",
    "\n",
    "In order to better identify neutral sentiment, you may make a threshold around zero score, for example (-0.1, +0.1)\n",
    "\n",
    "Below, you will find a code that outputs VADER scores for positive and negative questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "# nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "# Create text\n",
    "text_data = np.array(['I love Brazil. Brazil is best',\n",
    "                      'I like Italy, because Italy is beautiful',\n",
    "                      'Malaysia is ok, but I do not like spicy food',\n",
    "                      'I like Germany more, Germany beats all',\n",
    "                      'I do not like hot weather in Singapore'])\n",
    "X = text_data\n",
    "# Create target vector\n",
    "y = ['positive','positive','negative','positive','negative']\n",
    "\n",
    "# analyse with VADER\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "for text in text_data:\n",
    "    score = analyser.polarity_scores(text)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Complete the code\n",
    "\n",
    "\n",
    "Start with tweets_small.tsv dataset. When your code is working, then switch to tweets1.tsv, which will take longer to process.\n",
    "\n",
    "If you have time, you can experiment with other vectoriser types, like count vectorizer or WordVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import regex as re\n",
    "\n",
    "import spacy\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"tweets_small.tsv\", encoding=\"ISO-8859-1\", delimiter=\"\\t\", header=None)\n",
    "# data = pd.read_csv(\"tweets1.tsv\", encoding=\"ISO-8859-1\", delimiter=\"\\t\", header=None)\n",
    "data.columns = [\"text\", \"sentiment\"]\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove urls\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    # Remove '@' character using regex\n",
    "    text = <your code>\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    text = ''.join([char for char in text if ord(char) < 128])\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens =  <your code>\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = stopwords.words(\"english\") + list(string.punctuation)\n",
    "    tokens =  <your code>\n",
    "    text1 = \" \".join(tokens)\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    doc = nlp(text1)\n",
    "    tokens =  <your code>\n",
    "    \n",
    "    # Return the cleaned text as a string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply the function to the text column\n",
    "data[\"clean_text\"] =  <your code>\n",
    "\n",
    "# Display distribution of sentiments\n",
    "sns.countplot(x='sentiment', data=data)\n",
    "plt.title('Distribution of Sentiments')\n",
    "plt.show()\n",
    "\n",
    "# Analyze the distribution of tweet lengths\n",
    "data['tweet_length'] = data['clean_text'].apply(lambda x: len(x.split()))\n",
    "sns.histplot(data['tweet_length'], bins=20, kde=True)\n",
    "plt.title('Distribution of Tweet Lengths')\n",
    "plt.xlabel('Tweet Length')\n",
    "plt.show()\n",
    "\n",
    "# Function to get word frequencies for a given sentiment: either 0 (negative) or 1 (positive)\n",
    "# Use FreqDist function\n",
    "def get_word_frequencies(sentiment):\n",
    "    <your code>\n",
    "    return fdist\n",
    "\n",
    "# Plot word frequencies for positive sentiment\n",
    "positive_frequencies = get_word_frequencies(1)\n",
    "# plt.figure(figsize=(12, 6))\n",
    "plt.title('Word Frequencies for Positive Sentiments')\n",
    "positive_frequencies.plot(30, cumulative=False)\n",
    "# plt.show()\n",
    "\n",
    "# Plot word frequencies for negative sentiment\n",
    "negative_frequencies = get_word_frequencies(0)\n",
    "# plt.figure(figsize=(12, 6))\n",
    "plt.title('Word Frequencies for Negative Sentiments')\n",
    "negative_frequencies.plot(30, cumulative=False)\n",
    "# plt.show()\n",
    "\n",
    "# Split the data into train 70% and test sets 30%\n",
    "X_train, X_test, y_train, y_test = <your code>\n",
    "\n",
    "# Vectorize the text using TF-IDF default parameters\n",
    "vectorizer = TfidfVectorizer()\n",
    "# vectorizer = CountVectorizer(lowercase=True, token_pattern='[a-zA-Z0-9@#$%_]{2,}', \\\n",
    "#                                  max_features=None)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier default parameters\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Train a SVM classifier kernel and C=1\n",
    "svm = SVC(kernel=\"linear\", C=1)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models on the test set\n",
    "y_pred_mnb = <your code>\n",
    "y_pred_svm = <your code>\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_mnb))\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred_mnb))\n",
    "print(classification_report(y_test, y_pred_mnb))\n",
    "\n",
    "print(\"SVM:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In followng exercise you will experiment with unsupervised learning of sentiment. \n",
    "\n",
    "Since you know that there are two classes, positive and negative, you can set the number of clusters to 2.\n",
    "\n",
    "You will use two types of text representation: TF-IDF. and WordVectors. Implement both and see which one does better (using Silhouette score).\n",
    "\n",
    "Why do you think one is better than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text clustering\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# dataset\n",
    "text_list = data[\"clean_text\"]\n",
    "\n",
    "\n",
    "# feature extraction from clean_text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = <your code>\n",
    "\n",
    "# model training\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# cluster assignments\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "score = metrics.silhouette_score(X, clusters, metric='euclidean')\n",
    "print(\"Silhouette score for TF-IDF: \",score)\n",
    "\n",
    "# feature extraction and text vectorisation uing WordVectors\n",
    "tokens = <your code> # tokenise text_list \n",
    "model = Word2Vec(tokens, min_count=1)\n",
    "vectors = model.wv.vectors\n",
    "vectors.shape\n",
    "\n",
    "# # model training\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(vectors)\n",
    "\n",
    "# cluster assignments\n",
    "clusters = kmeans.labels_\n",
    "score = metrics.silhouette_score(vectors, clusters, metric='euclidean')\n",
    "print(\"Silhouette score for WordVectors: \",score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
