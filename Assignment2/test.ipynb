{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f30b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:58:24.987750: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-13 01:58:24.997747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747067305.008861   65743 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747067305.012688   65743 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747067305.021199   65743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747067305.021213   65743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747067305.021214   65743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747067305.021215   65743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-13 01:58:25.024341: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747067306.861185   65743 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6144 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Limit GPU memory usage\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.LogicalDeviceConfiguration(memory_limit=(6 * 1024))])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4729235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21dbf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_dim = 3          # e.g., service, cleanliness, value\n",
    "embedding_dim = 128\n",
    "lstm_units = 256\n",
    "vocab_size = 5000       # change depending on tokenizer\n",
    "max_seq_len = 20        # maximum length of output text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b1aaee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(rating_dim, embedding_dim, lstm_units, vocab_size, max_seq_len):\n",
    "    # Inputs\n",
    "    rating_input = layers.Input(shape=(rating_dim,), name=\"ratings\")\n",
    "    text_input = layers.Input(shape=(max_seq_len,), name=\"text\")\n",
    "\n",
    "    # Process ratings\n",
    "    rating_proj = layers.Dense(lstm_units, activation=\"relu\")(rating_input)\n",
    "    rating_proj = layers.RepeatVector(max_seq_len)(rating_proj)  # [batch, seq_len, lstm_units]\n",
    "\n",
    "    # Process tokens\n",
    "    text_embed = layers.Embedding(vocab_size, embedding_dim)(text_input)\n",
    "\n",
    "    # Combine ratings and text\n",
    "    lstm_input = layers.Concatenate()([text_embed, rating_proj])\n",
    "\n",
    "    # LSTM Decoder\n",
    "    lstm_output = layers.LSTM(lstm_units, return_sequences=True)(lstm_input)\n",
    "    output = layers.TimeDistributed(layers.Dense(vocab_size, activation=\"softmax\"))(lstm_output)\n",
    "\n",
    "    model = models.Model(inputs=[rating_input, text_input], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c799001a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ ratings             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ ratings[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │ text[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ repeat_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">656,384</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285,000</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ ratings             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ ratings[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m640,000\u001b[0m │ text[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m384\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ repeat_vector[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m656,384\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m5000\u001b[0m)  │  \u001b[38;5;34m1,285,000\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,582,408</span> (9.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,582,408\u001b[0m (9.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,582,408</span> (9.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,582,408\u001b[0m (9.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(rating_dim, embedding_dim, lstm_units, vocab_size, max_seq_len)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf96583",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "    {\"ratings\": [5.0, 5.0, 5.0], \"review\": \"Excellent service and very clean room.\"},\n",
    "    {\"ratings\": [4.0, 3.5, 4.5], \"review\": \"Good experience, but the room could be cleaner.\"},\n",
    "    {\"ratings\": [2.0, 2.5, 2.0], \"review\": \"Dirty room and poor service.\"},\n",
    "    {\"ratings\": [3.0, 4.0, 3.5], \"review\": \"Room was okay and fairly clean.\"},\n",
    "    {\"ratings\": [1.0, 1.5, 1.0], \"review\": \"Terrible experience. Not worth the money.\"},\n",
    "    {\"ratings\": [4.5, 4.5, 4.0], \"review\": \"Very clean and staff were friendly.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_json_to_df(file_name):\n",
    "    data = []\n",
    "    with open(file_name) as data_file:\n",
    "        for line in data_file:\n",
    "            # Load each line of the JSON file as a dictionary\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    # Form a Pandas DataFrame from the dictionaries\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "# Load the training and test data\n",
    "raw_train_df = read_json_to_df(\"hotel_reviews_train.json\")\n",
    "raw_test_df = read_json_to_df(\"hotel_reviews_test.json\")\n",
    "\n",
    "ratings_columns = [col for col in raw_train_df.columns if col.startswith(\"ratings.\")]\n",
    "\n",
    "# Select the title, text and overall rating columns to make a new dataframe\n",
    "train_df = raw_train_df[[\"title\", \"text\"] + ratings_columns]\n",
    "test_df = raw_test_df[[\"title\", \"text\"] + ratings_columns]\n",
    "\n",
    "# Save the English reviews to a CSV file to save time filtering when running again (NumFOCUS, Inc. 2024)\n",
    "if os.path.exists(\"english_hotel_reviews_train.csv\"):\n",
    "    train_df = pd.read_csv(\"english_hotel_reviews_train.csv\")\n",
    "\n",
    "if os.path.exists(\"english_hotel_reviews_test.csv\"):\n",
    "    test_df = pd.read_csv(\"english_hotel_reviews_test.csv\")\n",
    "\n",
    "train_df_2 = train_df.fillna(0)\n",
    "\n",
    "inputs = train_df_2[ratings_columns]\n",
    "# outputs = train_df_2['title'] + ' ' + train_df_2['text']\n",
    "outputs = train_df_2['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7f063d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ensure 'inputs' has no NaNs\n",
    "inputs = train_df_2[ratings_columns].fillna(0).astype(np.float32).values\n",
    "\n",
    "# Use review text as output\n",
    "outputs = train_df_2['text'].astype(str).values  # Ensure string type\n",
    "\n",
    "# Add special tokens\n",
    "texts_with_tokens = [\"<start> \" + text + \" <end>\" for text in outputs]\n",
    "\n",
    "# Tokenize text with special tokens\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts_with_tokens)\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts_with_tokens)\n",
    "\n",
    "# Define max length for padding\n",
    "max_len = 21  # or use max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Create input and target sequences (for teacher forcing in seq2seq models)\n",
    "input_seq = padded_sequences[:, :-1]\n",
    "target_seq = padded_sequences[:, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04d6497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ ratings_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ ratings_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,569,800</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">164</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ repeat_vector_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">431,104</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45698</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">11,744,386</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ ratings_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m640\u001b[0m │ ratings_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │  \u001b[38;5;34m4,569,800\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m164\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ repeat_vector_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m431,104\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m45698\u001b[0m) │ \u001b[38;5;34m11,744,386\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,745,930</span> (63.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,745,930\u001b[0m (63.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,745,930</span> (63.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,745,930\u001b[0m (63.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, RepeatVector, TimeDistributed\n",
    "\n",
    "# Key dimensions\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "seq_length = input_seq.shape[1]\n",
    "num_ratings = inputs.shape[1]\n",
    "\n",
    "# Ratings input branch\n",
    "ratings_input = Input(shape=(num_ratings,), name=\"ratings_input\")\n",
    "ratings_dense = Dense(64, activation='relu')(ratings_input)\n",
    "ratings_repeated = RepeatVector(seq_length)(ratings_dense)\n",
    "\n",
    "# Text input branch\n",
    "text_input = Input(shape=(seq_length,), name=\"text_input\")\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length)(text_input)\n",
    "\n",
    "# Combine both inputs\n",
    "merged = Concatenate()([embedding, ratings_repeated])\n",
    "\n",
    "# Decoder LSTM\n",
    "lstm_out = LSTM(256, return_sequences=True)(merged)\n",
    "output = TimeDistributed(Dense(vocab_size, activation='softmax'))(lstm_out)\n",
    "\n",
    "# Build and compile model\n",
    "model = Model(inputs=[ratings_input, text_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9082867",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq = np.expand_dims(target_seq, -1)  # shape: (samples, timesteps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fcf329e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m453/453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 85ms/step - loss: 6.6521 - val_loss: 6.0070\n",
      "Epoch 2/10\n",
      "\u001b[1m453/453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 86ms/step - loss: 5.8988 - val_loss: 5.6881\n",
      "Epoch 3/10\n",
      "\u001b[1m453/453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 87ms/step - loss: 5.6310 - val_loss: 5.5231\n",
      "Epoch 4/10\n",
      "\u001b[1m453/453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 85ms/step - loss: 5.4253 - val_loss: 5.1867\n",
      "Epoch 5/10\n",
      "\u001b[1m453/453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 85ms/step - loss: 5.0562 - val_loss: 4.9067\n",
      "Epoch 6/10\n",
      "\u001b[1m453/453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 86ms/step - loss: 4.7652 - val_loss: 4.7282\n",
      "Epoch 7/10\n",
      "\u001b[1m453/453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 86ms/step - loss: 4.5974 - val_loss: 4.6163\n",
      "Epoch 8/10\n",
      "\u001b[1m453/453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 86ms/step - loss: 4.4470 - val_loss: 4.5416\n",
      "Epoch 9/10\n",
      "\u001b[1m453/453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 85ms/step - loss: 4.3366 - val_loss: 4.4734\n",
      "Epoch 10/10\n",
      "\u001b[1m453/453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 87ms/step - loss: 4.2385 - val_loss: 4.4324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x794390c14560>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [inputs, input_seq],  # ratings + input tokens\n",
    "    target_seq,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00490bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_review(model, tokenizer, ratings_input, max_len=20, temperature=0.7, max_repeats=3):\n",
    "    # Get start/end token IDs safely\n",
    "    start_token = tokenizer.word_index.get('start')  \n",
    "    end_token = tokenizer.word_index.get('end')     \n",
    "\n",
    "    if start_token is None or end_token is None:\n",
    "        raise ValueError(\"Tokenizer is missing 'start' or 'end' tokens. Make sure you added them during training.\")\n",
    "\n",
    "    input_seq = [start_token]\n",
    "    generated_words = []  # To store the generated words\n",
    "    generated_token_ids = set()  # To track generated token IDs and avoid repetition\n",
    "\n",
    "    reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}  # Reverse index for decoding tokens\n",
    "\n",
    "    # Debugging: Check the reverse word index\n",
    "    print(f\"Reverse word index size: {len(reverse_word_index)}\")\n",
    "    print(f\"Reverse word index sample: {list(reverse_word_index.items())[:20]}\")  # Print first 20 items\n",
    "\n",
    "    repeat_count = 0  # Track repetition of words\n",
    "    for _ in range(max_len):\n",
    "        padded_seq = tf.keras.preprocessing.sequence.pad_sequences([input_seq], maxlen=max_len, padding='post')\n",
    "\n",
    "        preds = model.predict([ratings_input, padded_seq], verbose=0)\n",
    "        \n",
    "        # Apply temperature to the predictions\n",
    "        preds = preds[0][len(input_seq) - 1]  # Get prediction for the next word\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.exp(preds / temperature)  # Apply temperature\n",
    "        preds = preds / np.sum(preds)  # Normalize to sum to 1 (probabilities)\n",
    "\n",
    "        # Limit predictions to valid token IDs (tokens in the word index)\n",
    "        valid_tokens = list(tokenizer.word_index.values())  # Get list of all valid token IDs\n",
    "        valid_preds = preds[valid_tokens]  # Get the prediction probabilities for valid tokens\n",
    "        valid_preds /= np.sum(valid_preds)  # Normalize the valid tokens' probabilities\n",
    "\n",
    "        # Sample a token from the valid predictions\n",
    "        next_token_id = np.random.choice(valid_tokens, p=valid_preds)\n",
    "\n",
    "        # Map token ID to word using reverse_word_index\n",
    "        next_word = reverse_word_index.get(next_token_id, None)\n",
    "\n",
    "        # Debugging line: Check predicted token and its word\n",
    "        print(f\"Predicted token ID: {next_token_id} -> Word: {next_word}\")\n",
    "\n",
    "        # If the predicted word is invalid or None, skip this iteration or stop early\n",
    "        if next_word is None:  \n",
    "            print(\"Prediction is None, stopping early...\")\n",
    "            break\n",
    "\n",
    "        # Avoid repetition of the same token (in case of overly repetitive predictions)\n",
    "        if next_token_id in generated_token_ids:\n",
    "            repeat_count += 1\n",
    "        else:\n",
    "            repeat_count = 0\n",
    "        \n",
    "        if repeat_count > max_repeats:  # Stop if the model repeats the same word too much\n",
    "            print(\"Model is repeating tokens too often. Stopping early...\")\n",
    "            break\n",
    "\n",
    "        # Stop at end token\n",
    "        if next_token_id == end_token:\n",
    "            break\n",
    "\n",
    "        generated_words.append(next_word)\n",
    "        input_seq.append(next_token_id)\n",
    "        generated_token_ids.add(next_token_id)\n",
    "\n",
    "    # Convert list of words back to a string\n",
    "    generated_review = ' '.join(generated_words).strip()\n",
    "\n",
    "    print(f\"Generated review: {generated_review}\")\n",
    "    return generated_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bc46840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Reverse word index size: 45697\n",
      "Reverse word index sample: [(1, '<OOV>'), (2, 'the'), (3, 'and'), (4, 'a'), (5, 'to'), (6, 'was'), (7, 'i'), (8, 'in'), (9, 'we'), (10, 'of'), (11, 'is'), (12, 'for'), (13, 'hotel'), (14, 'it'), (15, 'room'), (16, 'at'), (17, 'were'), (18, 'but'), (19, 'with'), (20, 'on')]\n",
      "Predicted token ID: 17619 -> Word: clarified\n",
      "Predicted token ID: 13998 -> Word: kitschy\n",
      "Predicted token ID: 27637 -> Word: samething\n",
      "Predicted token ID: 42061 -> Word: midde\n",
      "Predicted token ID: 26217 -> Word: 2499\n",
      "Predicted token ID: 3683 -> Word: numbers\n",
      "Predicted token ID: 38493 -> Word: soonwe\n",
      "Predicted token ID: 41294 -> Word: sublease\n",
      "Predicted token ID: 31778 -> Word: josephine\n",
      "Predicted token ID: 23433 -> Word: joette\n",
      "Predicted token ID: 2527 -> Word: 250\n",
      "Predicted token ID: 27017 -> Word: irregular\n",
      "Predicted token ID: 16146 -> Word: lynn\n",
      "Predicted token ID: 26168 -> Word: blooks\n",
      "Predicted token ID: 35366 -> Word: peered\n",
      "Predicted token ID: 27879 -> Word: professionalone\n",
      "Predicted token ID: 40182 -> Word: wafts\n",
      "Predicted token ID: 13477 -> Word: vague\n",
      "Predicted token ID: 1175 -> Word: disappointing\n",
      "Predicted token ID: 35589 -> Word: selfcontained\n",
      "Generated review: clarified kitschy samething midde 2499 numbers soonwe sublease josephine joette 250 irregular lynn blooks peered professionalone wafts vague disappointing selfcontained\n"
     ]
    }
   ],
   "source": [
    "test_rating = np.array([[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]])  # Example rating\n",
    "print(seq_length)\n",
    "generated_review = generate_review(model, tokenizer, test_rating, max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32853b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/repos/COMP-SCI-7417-Applied-Natural-Language-Processing/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  Step 1: loss = 9.3640\n",
      "  Step 2: loss = 5.6177\n",
      "  Step 3: loss = 6.0542\n",
      "Epoch 2/100\n",
      "  Step 1: loss = 3.3473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:58:54.355631: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 2: loss = 1.9492\n",
      "  Step 3: loss = 2.3470\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:58:54.794185: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 1: loss = 1.7154\n",
      "  Step 2: loss = 0.5942\n",
      "  Step 3: loss = 0.3980\n",
      "Epoch 4/100\n",
      "  Step 1: loss = 0.4087\n",
      "  Step 2: loss = 0.3388\n",
      "  Step 3: loss = 0.4972\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:58:55.667729: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 1: loss = 0.2489\n",
      "  Step 2: loss = 0.2112\n",
      "  Step 3: loss = 0.3479\n",
      "Epoch 6/100\n",
      "  Step 1: loss = 0.3506\n",
      "  Step 2: loss = 0.2301\n",
      "  Step 3: loss = 0.2761\n",
      "Epoch 7/100\n",
      "  Step 1: loss = 0.2656\n",
      "  Step 2: loss = 0.1676\n",
      "  Step 3: loss = 0.1812\n",
      "Epoch 8/100\n",
      "  Step 1: loss = 0.1530\n",
      "  Step 2: loss = 0.1684\n",
      "  Step 3: loss = 0.2506\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:58:57.430874: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 1: loss = 0.2225\n",
      "  Step 2: loss = 0.1469\n",
      "  Step 3: loss = 0.1787\n",
      "Epoch 10/100\n",
      "  Step 1: loss = 0.1174\n",
      "  Step 2: loss = 0.2183\n",
      "  Step 3: loss = 0.1533\n",
      "Epoch 11/100\n",
      "  Step 1: loss = 0.1374\n",
      "  Step 2: loss = 0.1190\n",
      "  Step 3: loss = 0.1526\n",
      "Epoch 12/100\n",
      "  Step 1: loss = 0.1130\n",
      "  Step 2: loss = 0.1652\n",
      "  Step 3: loss = 0.1374\n",
      "Epoch 13/100\n",
      "  Step 1: loss = 0.1087\n",
      "  Step 2: loss = 0.0825\n",
      "  Step 3: loss = 0.1737\n",
      "Epoch 14/100\n",
      "  Step 1: loss = 0.0203\n",
      "  Step 2: loss = 0.1344\n",
      "  Step 3: loss = 0.1846\n",
      "Epoch 15/100\n",
      "  Step 1: loss = 0.1011\n",
      "  Step 2: loss = 0.0448\n",
      "  Step 3: loss = 0.1991\n",
      "Epoch 16/100\n",
      "  Step 1: loss = 0.0677\n",
      "  Step 2: loss = 0.0813\n",
      "  Step 3: loss = 0.0958\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:59:00.988269: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 1: loss = 0.0273\n",
      "  Step 2: loss = 0.1234\n",
      "  Step 3: loss = 0.0186\n",
      "Epoch 18/100\n",
      "  Step 1: loss = 0.0232\n",
      "  Step 2: loss = 0.0378\n",
      "  Step 3: loss = 0.0121\n",
      "Epoch 19/100\n",
      "  Step 1: loss = 0.0208\n",
      "  Step 2: loss = 0.0038\n",
      "  Step 3: loss = 0.0326\n",
      "Epoch 20/100\n",
      "  Step 1: loss = 0.0034\n",
      "  Step 2: loss = 0.0092\n",
      "  Step 3: loss = 0.0134\n",
      "Epoch 21/100\n",
      "  Step 1: loss = 0.0095\n",
      "  Step 2: loss = 0.0038\n",
      "  Step 3: loss = 0.0032\n",
      "Epoch 22/100\n",
      "  Step 1: loss = 0.0033\n",
      "  Step 2: loss = 0.0006\n",
      "  Step 3: loss = 0.0027\n",
      "Epoch 23/100\n",
      "  Step 1: loss = 0.0013\n",
      "  Step 2: loss = 0.0022\n",
      "  Step 3: loss = 0.0004\n",
      "Epoch 24/100\n",
      "  Step 1: loss = 0.0019\n",
      "  Step 2: loss = 0.0009\n",
      "  Step 3: loss = 0.0004\n",
      "Epoch 25/100\n",
      "  Step 1: loss = 0.0020\n",
      "  Step 2: loss = 0.0004\n",
      "  Step 3: loss = 0.0004\n",
      "Epoch 26/100\n",
      "  Step 1: loss = 0.0011\n",
      "  Step 2: loss = 0.0004\n",
      "  Step 3: loss = 0.0005\n",
      "Epoch 27/100\n",
      "  Step 1: loss = 0.0003\n",
      "  Step 2: loss = 0.0003\n",
      "  Step 3: loss = 0.0007\n",
      "Epoch 28/100\n",
      "  Step 1: loss = 0.0003\n",
      "  Step 2: loss = 0.0003\n",
      "  Step 3: loss = 0.0005\n",
      "Epoch 29/100\n",
      "  Step 1: loss = 0.0002\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0004\n",
      "Epoch 30/100\n",
      "  Step 1: loss = 0.0004\n",
      "  Step 2: loss = 0.0003\n",
      "  Step 3: loss = 0.0002\n",
      "Epoch 31/100\n",
      "  Step 1: loss = 0.0004\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0002\n",
      "Epoch 32/100\n",
      "  Step 1: loss = 0.0003\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0002\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:59:07.938961: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 1: loss = 0.0003\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0002\n",
      "Epoch 34/100\n",
      "  Step 1: loss = 0.0002\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0003\n",
      "Epoch 35/100\n",
      "  Step 1: loss = 0.0002\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0002\n",
      "Epoch 36/100\n",
      "  Step 1: loss = 0.0002\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0002\n",
      "Epoch 37/100\n",
      "  Step 1: loss = 0.0002\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 38/100\n",
      "  Step 1: loss = 0.0002\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 39/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0002\n",
      "Epoch 40/100\n",
      "  Step 1: loss = 0.0002\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 41/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0002\n",
      "Epoch 42/100\n",
      "  Step 1: loss = 0.0002\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 43/100\n",
      "  Step 1: loss = 0.0002\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 44/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0002\n",
      "Epoch 45/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 46/100\n",
      "  Step 1: loss = 0.0002\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 47/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 48/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0002\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 49/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 50/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 51/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 52/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 53/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 54/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 55/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 56/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 57/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 58/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 59/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 60/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 61/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 62/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 63/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 64/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:59:21.895739: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 66/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 67/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 68/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 69/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 70/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 71/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 72/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 73/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 74/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 75/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 76/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 77/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 78/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 79/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 80/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 81/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 82/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 83/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 84/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 85/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 86/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 87/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 88/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 89/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 90/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 91/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 92/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 93/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 94/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 95/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0000\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 96/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 97/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 98/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 99/100\n",
      "  Step 1: loss = 0.0001\n",
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Epoch 100/100\n",
      "  Step 1: loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 2: loss = 0.0001\n",
      "  Step 3: loss = 0.0001\n",
      "Generated review: RoomRoomRoom Room Room Room room room room and and andandandand and andAndandandAndand andand andAnd and andD and and... and. and and ...\n"
     ]
    }
   ],
   "source": [
    "# Text summarisation example\n",
    "from transformers import TFBartForConditionalGeneration, BartTokenizer, TFTrainingArguments\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def format_input(ratings):\n",
    "    return f\"service: {ratings[0]} cleanliness: {ratings[1]} value: {ratings[2]}\"\n",
    "\n",
    "inputs = [format_input(d[\"ratings\"]) for d in sample_data]\n",
    "targets = [d[\"review\"] for d in sample_data]\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = TFBartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Tokenize\n",
    "input_encodings = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"tf\")\n",
    "target_encodings = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"tf\")\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "labels = tf.where(\n",
    "    target_encodings.input_ids == tokenizer.pad_token_id,\n",
    "    tf.constant(IGNORE_INDEX, dtype=tf.int32),\n",
    "    target_encodings.input_ids,\n",
    ")\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        \"input_ids\": input_encodings.input_ids,\n",
    "        \"attention_mask\": input_encodings.attention_mask,\n",
    "        \"decoder_input_ids\": target_encodings.input_ids,\n",
    "        \"decoder_attention_mask\": target_encodings.attention_mask,\n",
    "    },\n",
    "    labels,\n",
    ")).shuffle(10).batch(2)\n",
    "\n",
    "# Compile model manually\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch_inputs, batch_labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(\n",
    "            input_ids=batch_inputs[\"input_ids\"],\n",
    "            attention_mask=batch_inputs[\"attention_mask\"],\n",
    "            decoder_input_ids=batch_inputs[\"decoder_input_ids\"],\n",
    "            decoder_attention_mask=batch_inputs[\"decoder_attention_mask\"],\n",
    "            labels=batch_labels,\n",
    "        )\n",
    "        loss = tf.reduce_mean(outputs.loss)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Train\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    for step, (batch_inputs, batch_labels) in enumerate(dataset):\n",
    "        loss = train_step(batch_inputs, batch_labels)\n",
    "        print(f\"  Step {step + 1}: loss = {loss.numpy():.4f}\")\n",
    "\n",
    "# Test generation\n",
    "test_input = \"service: 2.0 cleanliness: 1.0 value: 2.5\"\n",
    "test_encoding = tokenizer([test_input], return_tensors=\"tf\", padding=True, truncation=True)\n",
    "output_ids = model.generate(\n",
    "    input_ids=test_encoding[\"input_ids\"],\n",
    "    attention_mask=test_encoding[\"attention_mask\"],\n",
    "    max_length=50\n",
    ")\n",
    "generated_review = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated review:\", generated_review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
